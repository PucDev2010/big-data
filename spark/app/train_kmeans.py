from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lower, lit, coalesce
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import ClusteringEvaluator

# ====================================================
# 1. KH·ªûI T·∫†O SPARK SESSION
# ====================================================
spark = SparkSession.builder \
    .appName("JobClustering_KMeans") \
    .config("spark.cassandra.connection.host", "cassandra") \
    .config("spark.cassandra.connection.port", "9042") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# ====================================================
# 2. ƒê·ªåC D·ªÆ LI·ªÜU T·ª™ CASSANDRA
# ====================================================
print(">>> ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ Cassandra...")
df_raw = spark.read \
    .format("org.apache.spark.sql.cassandra") \
    .options(table="job_postings", keyspace="job_analytics") \
    .load()

print(f">>> T·ªïng s·ªë jobs: {df_raw.count()}")

# ====================================================
# 3. DATA PREPROCESSING
# ====================================================
print(">>> ƒêang x·ª≠ l√Ω d·ªØ li·ªáu...")

# L·ªçc b·ªè records r√°c
df = df_raw.filter(col("job_title").isNotNull())

# 3.1. X·ª≠ l√Ω L∆∞∆°ng
df = df.withColumn(
    "salary_final",
    coalesce(
        col("salary_avg"), 
        (col("salary_min") + col("salary_max")) / 2, 
        lit(0.0)
    )
)

# 3.2. X·ª≠ l√Ω Kinh nghi·ªám
df = df.withColumn(
    "exp_final",
    coalesce(col("exp_avg_year"), col("exp_min_year"), lit(0.0))
)

# 3.3. T·∫°o features t·ª´ text columns
# City features
df = df.withColumn("city_lower", lower(col("city")))
df = df.withColumn(
    "is_hcm",
    when(col("city_lower").rlike("h·ªì ch√≠ minh|hcm"), 1.0).otherwise(0.0)
)
df = df.withColumn(
    "is_hanoi",
    when(col("city_lower").rlike("h√† n·ªôi|ha noi|hanoi"), 1.0).otherwise(0.0)
)

# Job fields features
df = df.withColumn("job_fields_lower", lower(col("job_fields")))
df = df.withColumn(
    "is_it",
    when(col("job_fields_lower").rlike("it|ph·∫ßn m·ªÅm|developer|l·∫≠p tr√¨nh|data|ai"), 1.0).otherwise(0.0)
)
df = df.withColumn(
    "is_sales",
    when(col("job_fields_lower").rlike("b√°n h√†ng|kinh doanh|sales|ti·∫øp th·ªã|marketing"), 1.0).otherwise(0.0)
)
df = df.withColumn(
    "is_finance",
    when(col("job_fields_lower").rlike("t√†i ch√≠nh|ng√¢n h√†ng|k·∫ø to√°n|finance|banking"), 1.0).otherwise(0.0)
)
df = df.withColumn(
    "is_education",
    when(col("job_fields_lower").rlike("gi√°o d·ª•c|ƒë√†o t·∫°o|gi√°o vi√™n|education"), 1.0).otherwise(0.0)
)

# Position level features
df = df.withColumn("pos_lower", lower(col("position_level")))
df = df.withColumn(
    "is_manager",
    when(col("pos_lower").rlike("tr∆∞·ªüng|qu·∫£n l√Ω|gi√°m ƒë·ªëc|manager|lead|head"), 1.0).otherwise(0.0)
)
df = df.withColumn(
    "is_senior",
    when(col("pos_lower").rlike("senior|chuy√™n gia|chuy√™n vi√™n cao c·∫•p"), 1.0).otherwise(0.0)
)

# L·ªçc b·ªè jobs c√≥ d·ªØ li·ªáu b·∫•t h·ª£p l√Ω
# - salary = 0: Kh√¥ng c√≥ th√¥ng tin l∆∞∆°ng
# - salary > 200: L∆∞∆°ng > 200 tri·ªáu/th√°ng (kh√¥ng th·ª±c t·∫ø)
# - exp > 30: Kinh nghi·ªám > 30 nƒÉm (data b·ªã l·ªói)
df = df.filter(
    (col("salary_final") > 0) & 
    (col("salary_final") <= 200) &
    (col("exp_final") >= 0) &
    (col("exp_final") <= 30)
)

print(f">>> S·ªë jobs sau khi l·ªçc d·ªØ li·ªáu b·∫•t h·ª£p l√Ω: {df.count()}")

# ====================================================
# 4. FEATURE ENGINEERING CHO CLUSTERING
# ====================================================
feature_cols = [
    "salary_final",     # L∆∞∆°ng
    "exp_final",        # Kinh nghi·ªám
    "is_hcm",           # Th√†nh ph·ªë HCM
    "is_hanoi",         # Th√†nh ph·ªë H√† N·ªôi
    "is_it",            # Ng√†nh IT
    "is_sales",         # Ng√†nh Sales
    "is_finance",       # Ng√†nh T√†i ch√≠nh
    "is_education",     # Ng√†nh Gi√°o d·ª•c
    "is_manager",       # V·ªã tr√≠ qu·∫£n l√Ω
    "is_senior"         # V·ªã tr√≠ senior
]

# Fill null values v·ªõi 0
for col_name in feature_cols:
    df = df.fillna({col_name: 0.0})

# ====================================================
# 5. ML PIPELINE
# ====================================================
print(">>> ƒêang x√¢y d·ª±ng pipeline...")

# B∆∞·ªõc 1: Gom features th√†nh vector
assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features_raw"
)

# B∆∞·ªõc 2: Chu·∫©n h√≥a d·ªØ li·ªáu (quan tr·ªçng cho K-Means)
scaler = StandardScaler(
    inputCol="features_raw",
    outputCol="features",
    withStd=True,
    withMean=True
)

# B∆∞·ªõc 3: K-Means v·ªõi 5 clusters
# S·ªë K c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh d·ª±a tr√™n Elbow method ho·∫∑c Silhouette score
NUM_CLUSTERS = 5
kmeans = KMeans(
    k=NUM_CLUSTERS,
    featuresCol="features",
    predictionCol="cluster",
    seed=42
)

pipeline = Pipeline(stages=[assembler, scaler, kmeans])

# ====================================================
# 6. TRAIN MODEL
# ====================================================
print(f">>> ƒêang train K-Means v·ªõi K={NUM_CLUSTERS}...")
model = pipeline.fit(df)
print(">>> Train xong!")

# ====================================================
# 7. ƒê√ÅNH GI√Å MODEL
# ====================================================
# D·ª± ƒëo√°n cluster cho to√†n b·ªô data
predictions = model.transform(df)

# ƒê√°nh gi√° b·∫±ng Silhouette Score
evaluator = ClusteringEvaluator(
    featuresCol="features",
    predictionCol="cluster",
    metricName="silhouette"
)
silhouette = evaluator.evaluate(predictions)

print("\n" + "="*50)
print("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å MODEL")
print("="*50)
print(f"Silhouette Score: {silhouette:.4f}")
print("(Gi√° tr·ªã c√†ng g·∫ßn 1 c√†ng t·ªët, > 0.5 l√† kh√° t·ªët)")

# ====================================================
# 8. PH√ÇN T√çCH C√ÅC CLUSTER
# ====================================================
print("\n" + "="*50)
print("PH√ÇN B·ªê JOBS THEO CLUSTER")
print("="*50)
predictions.groupBy("cluster").count().orderBy("cluster").show()

# Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm t·ª´ng cluster
print("\n" + "="*50)
print("ƒê·∫∂C ƒêI·ªÇM TRUNG B√åNH C·ª¶A T·ª™NG CLUSTER")
print("="*50)

cluster_stats = predictions.groupBy("cluster").agg(
    {"salary_final": "avg", 
     "exp_final": "avg",
     "is_hcm": "avg",
     "is_hanoi": "avg",
     "is_it": "avg",
     "is_sales": "avg",
     "is_finance": "avg",
     "is_education": "avg",
     "is_manager": "avg",
     "is_senior": "avg"}
).orderBy("cluster")

cluster_stats.show(truncate=False)

# In m√¥ t·∫£ t·ª´ng cluster
print("\n" + "="*50)
print("M√î T·∫¢ T·ª™NG CLUSTER (D·ª∞A TR√äN ƒê·∫∂C ƒêI·ªÇM)")
print("="*50)

# L·∫•y d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch
stats_pd = cluster_stats.toPandas()
for _, row in stats_pd.iterrows():
    cluster_id = int(row['cluster'])
    salary = row['avg(salary_final)']
    exp = row['avg(exp_final)']
    
    # X√°c ƒë·ªãnh ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t
    features = []
    if row['avg(is_hcm)'] > 0.5:
        features.append("HCM")
    if row['avg(is_hanoi)'] > 0.5:
        features.append("H√† N·ªôi")
    if row['avg(is_it)'] > 0.3:
        features.append("IT")
    if row['avg(is_sales)'] > 0.3:
        features.append("Sales")
    if row['avg(is_finance)'] > 0.3:
        features.append("Finance")
    if row['avg(is_education)'] > 0.3:
        features.append("Education")
    if row['avg(is_manager)'] > 0.3:
        features.append("Manager")
    if row['avg(is_senior)'] > 0.3:
        features.append("Senior")
    
    # Ph√¢n lo·∫°i theo l∆∞∆°ng
    if salary >= 25:
        salary_level = "L∆∞∆°ng cao"
    elif salary >= 15:
        salary_level = "L∆∞∆°ng trung b√¨nh"
    else:
        salary_level = "L∆∞∆°ng th·∫•p"
    
    # Ph√¢n lo·∫°i theo kinh nghi·ªám
    if exp >= 3:
        exp_level = "Kinh nghi·ªám cao (3+ nƒÉm)"
    elif exp >= 1:
        exp_level = "Kinh nghi·ªám trung b√¨nh (1-3 nƒÉm)"
    else:
        exp_level = "Entry-level/Fresher"
    
    feature_str = ", ".join(features) if features else "ƒêa d·∫°ng"
    
    print(f"\nüìå Cluster {cluster_id}:")
    print(f"   - {salary_level} (~{salary:.1f} tri·ªáu)")
    print(f"   - {exp_level} (~{exp:.1f} nƒÉm)")
    print(f"   - ƒê·∫∑c ƒëi·ªÉm: {feature_str}")

# ====================================================
# 9. L∆ØU K·∫æT QU·∫¢ V√ÄO CASSANDRA
# ====================================================
print("\n>>> ƒêang l∆∞u k·∫øt qu·∫£ clustering v√†o Cassandra...")

# Ch·ªçn c√°c c·ªôt c·∫ßn l∆∞u
result_df = predictions.select(
    "id", "job_title", "city", "salary_final", "exp_final", 
    "job_fields", "position_level", "cluster"
)

# L∆∞u v√†o table m·ªõi
result_df.write \
    .format("org.apache.spark.sql.cassandra") \
    .option("keyspace", "job_analytics") \
    .option("table", "job_clusters") \
    .option("confirm.truncate", "true") \
    .mode("overwrite") \
    .save()

print(">>> ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o table job_analytics.job_clusters!")

# ====================================================
# 10. L∆ØU MODEL
# ====================================================
model_path = "/opt/spark/work-dir/models/job_clustering_kmeans"
model.write().overwrite().save(model_path)
print(f"\n>>> ƒê√£ l∆∞u model t·∫°i: {model_path}")

# ====================================================
# 11. HI·ªÇN TH·ªä M·∫™U K·∫æT QU·∫¢
# ====================================================
print("\n" + "="*50)
print("M·∫™U JOBS TRONG T·ª™NG CLUSTER")
print("="*50)

for i in range(NUM_CLUSTERS):
    print(f"\n--- Cluster {i} (5 jobs m·∫´u) ---")
    predictions.filter(col("cluster") == i) \
        .select("job_title", "city", "salary_final", "exp_final", "job_fields") \
        .show(5, truncate=50)

spark.stop()
print("\n‚úÖ HO√ÄN TH√ÄNH!")
