FROM apache/spark:3.5.3

USER root

# Install Java 17 (required for Spark 3.5.3)
# The base image may have Java 11, so we ensure Java 17 is installed
RUN apt-get update && \
    apt-get install -y openjdk-17-jdk && \
    update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-17-openjdk-amd64/bin/java 1 && \
    update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/java-17-openjdk-amd64/bin/javac 1 && \
    update-alternatives --set java /usr/lib/jvm/java-17-openjdk-amd64/bin/java && \
    update-alternatives --set javac /usr/lib/jvm/java-17-openjdk-amd64/bin/javac && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME to Java 17
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Verify Java installation
RUN java -version && echo "Java 17 installed successfully at $JAVA_HOME"

# Install numpy and other common ML dependencies
RUN pip3 install --no-cache-dir numpy pandas scipy scikit-learn

USER spark
