# üöÄ H∆∞·ªõng d·∫´n ch·∫°y ƒë·ªì √°n Big Data

## Y√™u c·∫ßu
- Docker Desktop ƒë√£ c√†i ƒë·∫∑t v√† ƒëang ch·∫°y

---

## B∆∞·ªõc 1: Kh·ªüi ƒë·ªông Docker containers

```bash
cd d:\UIT\hoc-ki-3\big-data\do-an\project
docker-compose up -d
```

Ki·ªÉm tra containers ƒëang ch·∫°y:
```bash
docker ps
```

Ph·∫£i th·∫•y 4 containers: `zookeeper`, `kafka`, `spark-master`, `spark-worker`, `cassandra`

---

## B∆∞·ªõc 2: ƒê·ª£i Cassandra kh·ªüi ƒë·ªông (~30-60 gi√¢y)

```bash
docker exec -it cassandra cqlsh -e "DESCRIBE KEYSPACES;"
```

N·∫øu th·∫•y l·ªói "Connection refused", ƒë·ª£i th√™m v√† th·ª≠ l·∫°i.

---

## B∆∞·ªõc 3: T·∫°o Keyspace v√† Table (ch·∫°y l·∫ßn ƒë·∫ßu)

```bash
docker exec -it cassandra cqlsh
```

Ch·∫°y c√°c l·ªánh CQL:
```sql
CREATE KEYSPACE IF NOT EXISTS job_analytics
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

USE job_analytics;

CREATE TABLE IF NOT EXISTS job_postings (
    id UUID PRIMARY KEY,
    job_title TEXT,
    job_type TEXT,
    position_level TEXT,
    city TEXT,
    experience TEXT,
    skills TEXT,
    job_fields TEXT,
    salary TEXT,
    salary_min DOUBLE,
    salary_max DOUBLE,
    salary_avg DOUBLE,
    unit TEXT,
    exp_min_year DOUBLE,
    exp_max_year DOUBLE,
    exp_avg_year DOUBLE,
    exp_type TEXT,
    event_time TIMESTAMP,
    event_type TEXT
);

-- Table l∆∞u k·∫øt qu·∫£ clustering
CREATE TABLE IF NOT EXISTS job_clusters (
    id UUID PRIMARY KEY,
    job_title TEXT,
    city TEXT,
    salary_final DOUBLE,
    exp_final DOUBLE,
    job_fields TEXT,
    position_level TEXT,
    cluster INT
);

EXIT;
```

---

## B∆∞·ªõc 4: Ch·∫°y Streaming ETL (ƒë·ªçc t·ª´ Kafka, ghi v√†o Cassandra)

```bash
docker exec -it spark-master /opt/spark/bin/spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 \
  /opt/spark/work-dir/job_streaming.py
```

---

## B∆∞·ªõc 5: M√¥ ph·ªèng d·ªØ li·ªáu (terminal kh√°c)

```bash
docker exec -it spark-master python3 /opt/spark/work-dir/real_time_data_simulation.py
```

---

## B∆∞·ªõc 6: Train model ML

### Option A: K-Means Clustering (Khuy·∫øn kh√≠ch)
```bash
docker exec spark-master /opt/spark/bin/spark-submit \
  --packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 \
  /opt/spark/work-dir/train_kmeans.py
```

### Option B: Logistic Regression
```bash
docker exec spark-master /opt/spark/bin/spark-submit \
  --packages com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 \
  /opt/spark/work-dir/train_logictis.py
```

---

## B∆∞·ªõc 7: Ch·∫°y Streamlit Dashboard (tr√™n m√°y host)

```bash
# C√†i ƒë·∫∑t dependencies
cd d:\UIT\hoc-ki-3\big-data\do-an\project\spark\app
pip install -r requirements.txt

# Ch·∫°y Streamlit
streamlit run streamlit_app.py
```

Truy c·∫≠p: http://localhost:8501

---

## Truy c·∫≠p c√°c d·ªãch v·ª•

| D·ªãch v·ª• | URL/Port |
|---------|----------|
| **Streamlit Dashboard** | http://localhost:8501 |
| Spark Master UI | http://localhost:8080 |
| Spark Worker UI | http://localhost:8081 |
| Cassandra | localhost:9042 (DataGrip/DBeaver) |
| Kafka | localhost:29092 |

---

## D·ª´ng t·∫•t c·∫£ containers

```bash
docker-compose down
```

X√≥a c·∫£ data:
```bash
docker-compose down -v
```
